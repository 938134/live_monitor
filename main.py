#!/usr/bin/env python3
import asyncio, httpx, json, logging, subprocess, sys
from datetime import datetime, timedelta, timezone
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s")

CFG = json.loads(Path("config.json").read_text(encoding="utf-8"))
PT_FILE   = CFG["files"]["pt_data"]
CH_FILE   = CFG["files"]["ch_data"]
URLS      = CFG["url"]["default"]
IGNORE    = set(CFG["url"]["ignore"])
KEYS      = CFG["keys"]
THRESHOLD = timedelta(hours=CFG["timeout"]["update_threshold"])
MAX_CONN  = CFG["concurrency"]["max"]

# ---------- å·¥å…· ----------
def log(msg): logging.info(msg)
def save(path, data): Path(path).write_text(json.dumps(data, ensure_ascii=False, indent=2))
def load(path):
    try:
        text = Path(path).read_text(encoding="utf-8").strip()
        return json.loads(text) if text else []
    except Exception:
        return []
# ---------- æ›´æ–° ----------
async def fetch(session, url):
    try:
        r = await session.get(url, timeout=CFG["timeout"]["web_request"])
        last = r.headers.get("Last-Modified")
        if last:
            mod = datetime.strptime(last, "%a, %d %b %Y %H:%M:%S %Z").replace(tzinfo=timezone.utc)
            if datetime.now(timezone.utc) - mod > THRESHOLD:
                return None
        return r.json()
    except Exception as e:
        log(f"âŒ fetch {url} {e}")
        return None

async def update_platforms(pt):
    sem = asyncio.Semaphore(MAX_CONN)
    async with httpx.AsyncClient() as session:
        tasks = [update_one(sem, session, p) for p in pt]
        await asyncio.gather(*tasks)

async def update_one(sem, session, p):
    async with sem:
        url = p[KEYS["address"]] + CFG["url"]["suffix"]
        data = await fetch(session, url)
        if not data:
            p[KEYS["result"]] = 0
            return
        for pf in data.get(KEYS["platform"], []):
            pf[KEYS["channel"]] = [
                ch for ch in pf.get(KEYS["channel"], [])
                if ch.get(KEYS["address"]) and ch[KEYS["address"]] not in IGNORE
            ]
        p[KEYS["platform"]] = data.get(KEYS["platform"], [])
        p[KEYS["result"]] = 1
        log(f"âœ… {url}")

# ---------- é¢‘é“å¯ç”¨æ£€æµ‹ ----------
def check_stream(url: str, timeout: int = 5) -> bool:
    """ä½¿ç”¨ ffmpeg æ£€æµ‹æµæ˜¯å¦åœ¨çº¿"""
    try:
        result = subprocess.run(
            ["ffmpeg", "-i", url, "-t", "1", "-f", "null", "-"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            timeout=timeout,
        )
        return result.returncode == 0
    except Exception:
        return False

def check_channels(channels):
    """å¤šçº¿ç¨‹æ£€æµ‹"""
    live = []
    total = len(channels)
    with ThreadPoolExecutor(max_workers=20) as pool:
        futures = [(ch, pool.submit(check_stream, ch[KEYS["address"]])) for ch in channels]
        for ch, fut in futures:
            ok = fut.result()
            log(f"{'âœ…' if ok else 'âŒ'} {ch[KEYS['name']]}")
            if ok:
                live.append(ch)
    log(f"æ£€æµ‹å®Œæˆï¼š{len(live)}/{total}")
    return live

# ---------- ä¸» ----------
async def main():
    log("ğŸš€ start")
    pt = load(PT_FILE)

    # åŒæ­¥ URL
    new_urls = set(URLS)
    old_urls = {x[KEYS["address"]] for x in pt}
    for u in new_urls - old_urls:
        pt.append({KEYS["address"]: u, KEYS["result"]: 0})

    await update_platforms(pt)
    save(PT_FILE, pt)

    # æ”¶é›†å…¨éƒ¨é¢‘é“
    all_channels = []
    for src in pt:
        if not src.get(KEYS["result"]):
            continue
        for pf in src.get(KEYS["platform"], []):
            if not pf.get(KEYS["result"]):
                continue
            all_channels.extend(pf.get(KEYS["channel"], []))

    # æ£€æµ‹å¯ç”¨
    live_channels = check_channels(all_channels)
    save(CH_FILE, live_channels)
    log(f"ğŸ“¦ saved {len(live_channels)} live channels")

if __name__ == "__main__":
    asyncio.run(main())